/* -*- Mode: C++; c-basic-offset:4; indent-tabs-mode:nil -*- */
/*
 * Copyright (c) 2022-2024 Triad National Security, LLC
 *                         All rights reserved.
 *
 * Copyright (c) 2022-2024 Inria
 *                         All rights reserved.
 *
 * Copyright (c) 2022-2024 Bordeaux INP
 *                         All rights reserved.
 *
 * This file is part of the quo-vadis project. See the LICENSE file at the
 * top-level directory of this distribution.
 */

/**
 * @file qvi-thread.cc
 */

#include "qvi-common.h" // IWYU pragma: keep
#include "qvi-thread.h"
#include "qvi-group.h"
#include "qvi-utils.h"
#ifdef OPENMP_FOUND
#include <omp.h>
#endif

// Type definitions.
typedef qvi_group_id_t qvi_thread_group_id_t;

using qvi_thread_group_tab_t = std::unordered_map<
    qvi_thread_group_id_t, qvi_thread_group_t
>;

// We need to have one structure for fields
// shared by all threads included in another
// with fields specific to each thread
struct qvi_thread_group_shared_s {
    /** Atomic reference count used for resource freeing. */
    std::atomic<int> in_use{0};
    /** UNUSED : ID (rank) in group */
    /* probably need a LIST of IDs ...*/
    int id = -1;
    /** ID used for table lookups. */
    qvi_thread_group_id_t tabid = 0;
    /** Group size. */
     int size = 0;
    /** Barrier object (used in scope). */
    pthread_barrier_t barrier;
};

struct qvi_thread_group_s {
    /**
     * Shared data between threads in the group:
     * ALL threads point to the same region.
     */
    qvi_thread_group_shared_t *sdata = nullptr;
    /** ID (rank) in group: this ID is unique to each thread. */
    int id = 0;
};

struct qvi_thread_s {
    /** Group table (ID to internal structure mapping) */
    qvi_thread_group_tab_t *group_tab = nullptr;
    /** Constructor. */
    qvi_thread_s(void)
    {
        // Groups
        const int rc = qvi_new(&group_tab);
        if (rc != QV_SUCCESS) throw qvi_runtime_error();
    }
    /** Destructor. */
    ~qvi_thread_s(void)
    {
        qvi_delete(&group_tab);
    }
};

/**
 * Copies contents of internal structure from src to dst.
 */
// TODO(skg) Do we need this?
static void
cp_thread_group(
    const qvi_thread_group_t *src,
    qvi_thread_group_t *dst
) {
    memmove(dst, src, sizeof(*src));
}

/**
 *
 */
static int
next_group_tab_id(
    qvi_thread_t *,
    qvi_thread_group_id_t *gid
) {
    return qvi_group_t::next_id(gid);
}

/**
 *
 */
int
qvi_thread_new(
    qvi_thread_t **th
) {
    return qvi_new(th);
}

/**
 *
 */
void
qvi_thread_free(
    qvi_thread_t **th
) {
    qvi_delete(th);
}

/**
 *
 */
int
qvi_thread_finalize(
    qvi_thread_t *
) {
    return QV_SUCCESS;
}

/**
 *
 */
int
qvi_thread_group_new(
    qvi_thread_group_t **thgrp
) {
    qvi_thread_group_t *ithgrp = new qvi_thread_group_t();
    qvi_thread_group_shared_t *sdata = nullptr;

#pragma omp single copyprivate(sdata)
    sdata = new qvi_thread_group_shared_t();

#pragma omp single
    sdata->in_use = omp_get_num_threads();

    ithgrp->sdata = sdata;

    *thgrp = ithgrp;
    return QV_SUCCESS;
}

/**
 *
 */
void
qvi_thread_group_free(
    qvi_thread_group_t **thgrp
) {
    qvi_thread_group_t *ithgrp = *thgrp;
    if (!ithgrp) goto out;

    if (--(ithgrp->sdata->in_use) == 0) {
      pthread_barrier_destroy(&(ithgrp->sdata->barrier));
      delete ithgrp->sdata;
    }

    delete ithgrp;
out:
    *thgrp = nullptr;
}

/**
 *
 */
static int
qvi_thread_group_create_size(
    qvi_thread_t *th,
    qvi_thread_group_t **group,
    int size
) {
    qvi_thread_group_t *igroup = nullptr;
    qvi_thread_group_id_t gtid = 0;
    int rc, irc;

    rc = qvi_thread_group_new(&igroup);
    if (rc != QV_SUCCESS) {
      *group = nullptr;
      return rc;
    }

#pragma omp single copyprivate(gtid, rc)
    rc = next_group_tab_id(th, &gtid);
    if (rc != QV_SUCCESS) goto out;

#pragma omp single
    {
      igroup->sdata->tabid = gtid;
      igroup->sdata->size = size;
    }
#ifdef OPENMP_FOUND
    igroup->id = omp_get_thread_num();
#else
    igroup->id = 0;
#endif

#pragma omp single copyprivate(irc)
    irc = pthread_barrier_init(&(igroup->sdata->barrier),NULL,igroup->sdata->size);
    if (irc != 0) {
      rc = QV_ERR_INTERNAL;
      goto out;
    }

    //Insert the whole group structure or just the shared part??
#pragma omp single
    th->group_tab->insert({gtid, *igroup});

out:
    if (rc != QV_SUCCESS) {
      qvi_thread_group_free(&igroup);
    }

    *group = igroup;
    return QV_SUCCESS;
}

/**
 *
 */
int
qvi_thread_group_create(
    qvi_thread_t *th,
    qvi_thread_group_t **group
) {
#ifdef OPENMP_FOUND
    return qvi_thread_group_create_size(th, group, omp_get_num_threads());
#else
    return qvi_thread_group_create_size(th, group, 1); /* Size to fix here */
#endif
}

/**
 *
 */
int
qvi_thread_group_create_single(
    qvi_thread_t *th,
    qvi_thread_group_t **group
) {
    return qvi_thread_group_create_size(th, group, 1);
}

/**
 *
 */
int
qvi_thread_group_lookup_by_id(
    qvi_thread_t *th,
    qvi_thread_group_id_t id,
    qvi_thread_group_t *group
) {
    auto got = th->group_tab->find(id);
    if (got == th->group_tab->end()) {
        return QV_ERR_NOT_FOUND;
    }
    cp_thread_group(&got->second, group);
    return QV_SUCCESS;
}

/**
 *
 */
int
qvi_thread_group_id(
    const qvi_thread_group_t *group
) {
    return group->id;
}

/**
 *
 */
int
qvi_thread_group_size(
    const qvi_thread_group_t *group
) {
    return group->sdata->size;
}

/**
 *
 */
int
qvi_thread_group_barrier(
    qvi_thread_group_t *group
) {
    const int rc = pthread_barrier_wait(&(group->sdata->barrier));
    if ((rc != 0) && (rc != PTHREAD_BARRIER_SERIAL_THREAD)) {
        return QV_ERR_INTERNAL;
    }
    return QV_SUCCESS;
}

#ifdef OPENMP_FOUND
/**
 * Internal data structure for rank and size computing.
 */
typedef struct color_key_id_s {
    int color;
    int key;
    int rank;
} color_key_id_t;

/**
 *
 */
static void
swap_elts(
    color_key_id_t *el1,
    color_key_id_t *el2
) {
    const color_key_id_t tmp = *el1;
    *el1 = *el2;
    *el2 = tmp;
}

/**
 *
 */
static void
bubble_sort_by_color(
    color_key_id_t tab[],
    int size_tab
) {
    if (size_tab > 1) {
        for(int i = 0 ; i < size_tab - 1 ; i++) {
            if (tab[i].color > tab[i+1].color) {
                swap_elts(&tab[i],&tab[i+1]);
            }
        }
        bubble_sort_by_color(tab, size_tab - 1);
    }
}

/**
 *
 */
static void
bubble_sort_by_key(
    color_key_id_t tab[],
    int size_tab
) {
    if (size_tab > 1) {
      for(int i = 0 ; i < size_tab - 1 ; i++) {
          if ((tab[i].color == tab[i+1].color) &&
              (tab[i].key > tab[i+1].key)) {
              swap_elts(&tab[i],&tab[i+1]);
          }
      }
      bubble_sort_by_key(tab,size_tab-1);
    }
}

/**
 *
 */
static void bubble_sort_by_rank(
    color_key_id_t tab[],
    int size_tab
) {
    if (size_tab > 1) {
        for(int i = 0 ; i < size_tab - 1 ; i++) {
            if ((tab[i].color == tab[i+1].color) &&
                (tab[i].key == tab[i+1].key)     &&
                (tab[i].rank > tab[i+1].rank)) {
                swap_elts(&tab[i],&tab[i+1]);
            }
        }
        bubble_sort_by_key(tab, size_tab - 1);
    }
}

/**
 *
 */
/*
static void display_tab(
    pid_t tid,
    color_key_id_t lptr[],
    int size
) {
    fprintf(stdout,"=============================================================================\n");
    fprintf(stdout, "[%6i] COLORS = ",tid);
    for(int i = 0 ; i < size ; i++)
       fprintf(stdout, "[%i]:%6i | ", i,lptr[i].color);
    fprintf(stdout,"\n");
    fprintf(stdout, "[%6i] KEYS   = ",tid);
    for(int i = 0 ; i < size ; i++)
       fprintf(stdout, "[%i]:%6i | ", i,lptr[i].key);
    fprintf(stdout,"\n");
    fprintf(stdout, "[%6i] RANK   = ",tid);
    for(int i = 0 ; i < size ; i++)
       fprintf(stdout, "[%i]:%6i | ", i,lptr[i].rank);
    fprintf(stdout,"\n");
    fprintf(stdout,"\n");
    fprintf(stdout,"=============================================================================\n");
    fprintf(stdout,"\n\n");
}
*/
#endif

/**
 *
 */
static int
qvi_get_subgroup_info(
    const qvi_thread_group_t *parent,
    int color,
    int key,
    int *new_id,
    int *sgrp_size,
    int *sgrp_rank,
    int *num_of_sgrp
) {
    int color_val= 0;
    int num_colors = 0;
    int idx = 0;
    int idx2 = 0;
    int size = parent->sdata->size;
    int id = parent->id;
    color_key_id_t *lptr = NULL;

#pragma omp single copyprivate(lptr)
    lptr = new color_key_id_t[size]();

    /* Gather colors and keys from ALL threads */
    lptr[id].color = color;
    lptr[id].key   = key;
    lptr[id].rank  = id;
#pragma omp barrier // to be sure that all threads have contributed

    /* Sort the color/key/rank  array, according to color first, */
    /* then to key, but in the same color realm  */
    /* if color and key are identical, sort by rank in old group */
#pragma omp single  /* since this data is shared, only one thread has to sort it */
    {
      bubble_sort_by_color(lptr, size);
      bubble_sort_by_key(lptr, size);
      bubble_sort_by_rank(lptr, size);
    }

    /* compute number of subgroups */
    num_colors = 1;
    color_val = lptr[0].color;
    for(int idx = 0 ; idx < size ; idx++) {
      if(lptr[idx].color != color_val) {
          num_colors++;
          color_val = lptr[idx].color;
      }
    }
    *num_of_sgrp = num_colors;

    /* compute number and ranks of subgroups */
    num_colors = 0;
    color_val = lptr[0].color;
    for(int idx = 0 ; idx < size ; idx++) {
      if(lptr[idx].color != color_val) {
    num_colors++;
    color_val = lptr[idx].color;
      }
      if(lptr[idx].color == color) {
    *sgrp_rank = num_colors;
    break;
      }
    }

    /* Compute subgroup size and thread id in subgroup */
    /* 1- Move to the right part in the array */
    idx = 0;
    while((idx < size) && (lptr[idx].color != color))
      idx++;

    /* 2- Compute the subgroup size */
    idx2 = idx;
    while((idx2 < size) && (lptr[idx2].color == color))
      idx2++;
    *sgrp_size = (idx2 - idx);

    /* 3- Compute id in the subgroup */
    idx2 = idx;
    while((idx2 < size) && (lptr[idx2].rank != id))
      idx2++;
    *new_id = (idx2 - idx);

#pragma omp barrier // to prevent the quickest thread to remove data before all others have used it
#pragma omp single
    delete [] lptr;
    return QV_SUCCESS;
}

/**
 *
 */
int
qvi_thread_group_create_from_split(
    qvi_thread_t *th,
    const qvi_thread_group_t *parent,
    int color,
    int key,
    qvi_thread_group_t **child
)
#ifndef OPENMP_FOUND
{
    *child = nullptr;
    return QV_SUCCESS;
}
#else
{
    int rc = QV_SUCCESS;
    int newid = -1;     /* thread new id in subgroup */
    int sgrp_size = -1; /* size of this thread subgroup */
    int sgrp_rank = -1; /* subgroup rank */
    int num_sgrp = -1;  /* number of subgroups */
    qvi_thread_group_t *new_group = nullptr;
    qvi_thread_group_id_t gtid;

    /* this used to internally allocate things */
    /* in a copyprivate fashion */
    //qvi_thread_group_t **new_group_ptr_array = nullptr;
    //qvi_thread_group_t *tmp_group = nullptr;
    qvi_thread_group_shared_t **sdata_ptr_array = nullptr;
    qvi_thread_group_shared_t *tmp_sdata = nullptr;
    omp_lock_t *lock_ptr = nullptr; // use pthread_mutex instead ??*/

    if (!parent) {
      *child = nullptr;
      return QV_ERR_SPLIT;
    }

    rc = qvi_get_subgroup_info(
     parent,
     color,
     key,
     &newid,
     &sgrp_size,
     &sgrp_rank,
     &num_sgrp );
    if (rc != QV_SUCCESS) {
      *child = nullptr;
      return QV_ERR_SPLIT;
    }

    /* create and init each subgroup : qvi_thread_group_new canNOT be used */
    /* since it already contains a "single" clause. */
    /* Calling the function directly will deadlock. */
    /* The same goes for qvi_thread_group_create. */
    /* Do everything manually */

    /* Equivalent to (sub)group "new" (cf qvi_thread_group_new) */
    new_group = new qvi_thread_group_t();

    /* sdata pointer allocation */
#pragma omp single copyprivate(sdata_ptr_array)
    sdata_ptr_array = new qvi_thread_group_shared_t *[num_sgrp]();

    for(int i = 0 ; i < num_sgrp ; i++) {
#pragma omp single copyprivate(tmp_sdata)
        tmp_sdata = new qvi_thread_group_shared_t();
        /* Since it's shared, only the root(s) need(s) to set this */
        if ((newid == 0) && (i == sgrp_rank)) {
            sdata_ptr_array[i] = tmp_sdata;
        }
    }

#pragma omp single copyprivate(lock_ptr)
    {
      lock_ptr = new omp_lock_t();
      omp_init_lock(lock_ptr);
    }

    /* Only the root(s) need(s) to initialize this */
    if (newid == 0) {
      omp_set_lock(lock_ptr);
      rc = next_group_tab_id(th, &gtid);
      omp_unset_lock(lock_ptr);

      sdata_ptr_array[sgrp_rank]->tabid = gtid;

      sdata_ptr_array[sgrp_rank]->size = sgrp_size;
      sdata_ptr_array[sgrp_rank]->in_use = sgrp_size;

      pthread_barrier_init(&(sdata_ptr_array[sgrp_rank]->barrier),NULL,sgrp_size);
    }
#pragma omp barrier
    /* All threads set their own (sub)group sdata pointer */
    new_group->sdata = sdata_ptr_array[sgrp_rank];
    new_group->id = newid;

    /* Only the root(s) need(s) to insert groups */
    if (newid == 0) {
      omp_set_lock(lock_ptr);
      th->group_tab->insert({gtid, *new_group});
      omp_unset_lock(lock_ptr);
    }

    *child = new_group;
#pragma omp barrier // to prevent the quickest thread to remove data before all others have used it
#pragma omp single
    {
      delete lock_ptr;
      delete [] sdata_ptr_array;
    }
    return rc;
}
#endif

/**
 *
 */
int
qvi_thread_group_gather_bbuffs(
    qvi_thread_group_t *group,
    qvi_bbuff_t *txbuff,
    int root,
    qvi_bbuff_t ***rxbuffs,
    int *shared_alloc
) {
    QVI_UNUSED(root);
    const int send_count = (int)qvi_bbuff_size(txbuff);
    const int group_size = group->sdata->size;
    const int group_id   = group->id;
    int rc = QV_SUCCESS;
    qvi_bbuff_t **bbuffs = nullptr;
    // Zero initialize array of pointers to nullptr.
#pragma omp single copyprivate(bbuffs) //shared bbuffs allocation
    bbuffs = new qvi_bbuff_t *[group_size]();

    rc = qvi_bbuff_new(&bbuffs[group_id]);
    if (rc != QV_SUCCESS) goto out;

    rc = qvi_bbuff_append(bbuffs[group_id], qvi_bbuff_data(txbuff), send_count);
    if (rc != QV_SUCCESS) goto out;

#pragma omp barrier // Need to ensure that all threads have contributed to bbuffs

out:
    if (rc != QV_SUCCESS) {
#pragma omp single
      {
        if (bbuffs) {
            for (int i = 0; i < group_size; ++i) {
                qvi_bbuff_free(&bbuffs[i]);
            }
            delete[] bbuffs;
        }
      }
      bbuffs = nullptr;
    }
    *rxbuffs = bbuffs;
    *shared_alloc = 1;
    //for(int i = 0 ; i < group_size ; i++)
    //fprintf(stdout,"===== [%i] bbuff[%i] à %p\n",group_id,i,bbuffs[i]);
    return rc;
}

/**
 *
 */
int
qvi_thread_group_scatter_bbuffs(
    qvi_thread_group_t *group,
    qvi_bbuff_t **txbuffs,
    int root,
    qvi_bbuff_t **rxbuff
) {
    QVI_UNUSED(root);
    const int group_id = group->id;
    qvi_bbuff_t  ***tmp = nullptr;

    /* GM: Oh man, that is UGLY */
    /* Surely, some nice OpenMP magic will fix this mess */
#pragma omp single copyprivate(tmp)
    tmp = new qvi_bbuff_t**();
#pragma omp master
    *tmp = txbuffs;
#pragma omp barrier

    //fprintf(stdout,">>>>>>>>>>>>>>>>>>>>>>> SCATTER [%i] tmp @ %p txbuffs @ %p\n",group_id,*tmp,(*tmp)[group_id]);

    qvi_bbuff_t *inbuff = (*tmp)[group_id];
    size_t data_size = qvi_bbuff_size(inbuff);
    void *data = qvi_bbuff_data(inbuff);

    qvi_bbuff_t *mybbuff = nullptr;
    int rc = qvi_bbuff_new(&mybbuff);
    if (rc == QV_SUCCESS) {
      rc = qvi_bbuff_append(mybbuff, data, data_size);
    }
#pragma omp barrier
#pragma omp single
    delete tmp;

    if (rc != QV_SUCCESS) {
        qvi_bbuff_free(&mybbuff);
    }
    *rxbuff = mybbuff;
    //fprintf(stdout,">>>>>>>>>>>>>>>>>>>>>>> SCATTER [%i] OUT with RC %i\n",group_id,rc);
    return rc;
}

/*
 * vim: ft=cpp ts=4 sts=4 sw=4 expandtab
 */
